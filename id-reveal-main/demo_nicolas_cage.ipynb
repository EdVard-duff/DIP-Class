{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements: pytorch>=1.6.0 numpy tqdm matplotlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from network import IDreveal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device   = 'cpu' # in ('cpu', 'cuda:0', 'cuda:1')\n",
    "time     = 100   # length of sequences in frames\n",
    "\n",
    "ref_vids = [ 'real/vid25fps_caxMBk1__-Y', ] # Reference Videos\n",
    "\n",
    "test_vids =  [ # Test Videos \n",
    "    #(Video, Type),\n",
    "    ('real/vid25fps_Z1JyukEGjb0',  0),\n",
    "    ('real/vid25fps_GdxofSvTYUI',  0),\n",
    "    ('real/vid25fps_M0iV5vIABX0',  0),\n",
    "    ('fake/vid25fps_oLih6bDkmqg',  1),\n",
    "    ('fake/vid25fps_kqKgCB4hJw4',  1),\n",
    "    ('fake/vid25fps_dh-QM54RuAs',  1),\n",
    "    ('fake/vid25fps_w8sdYZjs-1I',  1),\n",
    "    ('real/vid25fps_JdA9_mtXYME',  2),\n",
    "    ('fake/vid25fps_4hMa-gKljhw',  3),\n",
    "]\n",
    "\n",
    "typ_colors = ['C2', 'C3', 'C4', 'C6'] # color for each type of video\n",
    "typ_labels = ['Real videos', 'Deepfakes', 'Imitator', 'Deepfake on the imitator'] # label for each type of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IDreveal(time=time, device=device, weights_file='./model.th')\n",
    "def extract_embedding(x):\n",
    "    if isinstance(x, str):\n",
    "        x = np.load(x) # load 3ddfa features\n",
    "    \n",
    "    # insert NAN in the temporal positions where the face is not detected\n",
    "    ts = int(np.nanmin(x['image_inds']))\n",
    "    te = int(np.nanmax(x['image_inds'])+1)\n",
    "    inp = np.full((te-ts, x['3ddfa'].shape[1]), np.nan, dtype=np.float32)\n",
    "    for i,d in zip(x['image_inds'], x['3ddfa']):\n",
    "        if np.isfinite(i):\n",
    "            inp[int(i)-ts] = d\n",
    "    \n",
    "    y = net(inp) # apply Temporal ID Network\n",
    "    y = y[np.all(np.isfinite(y),-1)] # remove NAN positions\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract embedded vectors for reference videos\n",
    "print('Extracting embedded vectors for reference videos', flush=True)\n",
    "ref_embs = np.concatenate([extract_embedding('./feats/%s.npz' % vid) for vid in tqdm(ref_vids)], 0) # 2149，128\n",
    "print(flush=True)\n",
    "print('Number of reference embedded vectors:', len(ref_embs), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_embs = extract_embedding('./feats/real/vid25fps_Z1JyukEGjb0.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2419, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1428, 2419, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_embs.shape\n",
    "tar_embs.ndim\n",
    "dis = ref_embs[None,:,:] - tar_embs[:,None,:]\n",
    "dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(4).reshape(2,2)\n",
    "b = a[None,:,:]\n",
    "b.shape\n",
    "c = a[:,None,:]\n",
    "c.shape\n",
    "d = b-c\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting embedded vectors and distance computation for test videos', flush=True)\n",
    "list_dist = list()\n",
    "count_embs = 0\n",
    "for vid, typ in tqdm(test_vids):\n",
    "    # 维数 x,128\n",
    "    embs = extract_embedding('./feats/%s.npz' % vid) # extract embedded vectors for a test video\n",
    "    count_embs = count_embs + len(embs)\n",
    "    ## 欧氏距离计算\n",
    "    dist = np.min(np.sum(np.square(ref_embs[None,:,:]-embs[:,None,:]),-1),-1) # compute distances\n",
    "    list_dist.append((dist, typ)) # 储存了距离和类型\n",
    "print(flush=True)\n",
    "print('Total number of extracted vectors:', count_embs, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_color_violin(parts, color): #part 是箱线图的一个组件\n",
    "    #utlity function to change the color of violin-plot\n",
    "    ret = None\n",
    "    for keys in parts:\n",
    "        if keys=='bodies':\n",
    "            for pc in parts['bodies']:\n",
    "                ret = pc\n",
    "                pc.set_facecolor(color)\n",
    "                pc.set_edgecolor(color)\n",
    "        else:\n",
    "            parts[keys].set_edgecolor(color)\n",
    "            parts[keys].set_facecolor(color)\n",
    "    return ret\n",
    "\n",
    "# show violin-plot\n",
    "plt.figure(figsize=(14,6))\n",
    "typ_id = [None for _ in typ_labels] # 预定义成长为4的列表\n",
    "for i, (dist, typ) in enumerate(list_dist):\n",
    "    parts = plt.violinplot(dist, positions=(i+0.5,), showmedians=True, points=dist.size)\n",
    "    typ_id[typ] = set_color_violin(parts, typ_colors[typ])\n",
    "\n",
    "plt.xlim([0,len(list_dist)])\n",
    "plt.xticks(np.arange(len(list_dist)),[])\n",
    "plt.ylim([0.5,5.5])\n",
    "plt.yticks([1,2,3,4,5],[], fontsize=10.0)\n",
    "plt.ylabel('Squared Euclidean distance', fontsize=14.0)\n",
    "\n",
    "plt.legend(typ_id,  typ_labels, bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=12.0)\n",
    "plt.grid()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_id = [None for _ in typ_labels]\n",
    "typ_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb71fd9c3a8870bae8b470c3c4c52a9514a1276bbeb9ea5c4061a9c129ee4266"
  },
  "kernelspec": {
   "display_name": "torch1.6.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
